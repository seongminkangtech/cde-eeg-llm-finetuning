"""
Unsloth ê¸°ë°˜ íŒŒì¸íŠœë‹ íŠ¸ë ˆì´ë„ˆ

- Unsloth ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹
- ê¸°ì¡´ config ì¹´í…Œê³ ë¦¬ì™€ í˜¸í™˜ë˜ëŠ” êµ¬ì¡°
- í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë¶„ë¥˜ íƒœìŠ¤í¬ ì§€ì›
- MLflow í†µí•© ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ìë™ í‰ê°€
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ í†µí•œ ìˆ˜ë™ Early Stopping
"""

from __future__ import annotations

import torch
import pandas as pd
from pathlib import Path
from typing import Dict
from tqdm import tqdm
import time
import signal
import sys
from sklearn.metrics import classification_report, confusion_matrix

from unsloth import FastLanguageModel
from datasets import load_dataset
from transformers import TrainingArguments, TrainerCallback
from trl import SFTTrainer
import mlflow
import mlflow.pytorch
from datasets import Dataset

from .config import ModelConfig, DataConfig, TrainerConfig


# ============================================================================
# ì „ì—­ ë³€ìˆ˜ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================

# ì „ì—­ ë³€ìˆ˜ë¡œ Early Stopping í”Œë˜ê·¸ ê´€ë¦¬
early_stop_requested = False

def cleanup_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ (GPU/CPU ëª¨ë‘ ì ìš©)"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    import gc
    gc.collect()


# ============================================================================
# í”„ë¡¬í”„íŠ¸ ìƒì„± ë° í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================================

def create_train_prompt(text: str, label: str) -> str:
    """EEG ë¶„ë¥˜ë¥¼ ìœ„í•œ í›ˆë ¨ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±."""
    
    instruction = "Analyze the given EEG report and classify the status based on the following categories:"
    
    prompt = f"""### Instruction:
{instruction}

### EEG Report:
{text}

### Classification:
{label}<|endoftext|>"""
    
    return prompt

def create_test_prompt(text: str) -> str:
    """í…ŒìŠ¤íŠ¸ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± (ë ˆì´ë¸” ì—†ì´)"""
    
    instruction = "Analyze the given EEG report and classify the status based on the following categories:"
    
    prompt = f"""### Instruction:
{instruction}

### EEG Report:
{text}

### Classification:"""
    
    return prompt

def extract_classification_label(predicted_text: str) -> str:
    """ëª¨ë¸ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸ì—ì„œ ë¶„ë¥˜ ë ˆì´ë¸”ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    
    try:
        # ê°€ëŠ¥í•œ ë ˆì´ë¸”ë“¤
        LABELS = ['Abnormal(Nonspecific)', 'Abnormal(Interictal)', 'Abnormal(Ictal)', 'Normal']
        
        # "### Classification:" ì´í›„ì˜ í…ìŠ¤íŠ¸ì—ì„œ ë ˆì´ë¸” ì°¾ê¸°
        if "### Classification:" in predicted_text:
            classification_part = predicted_text.split("### Classification:")[-1].strip()
            
            
            for label in LABELS:
                if label.lower() in classification_part.lower():
                    print(f"Debug - ë ˆì´ë¸” ì¶”ì¶œ: {label}")
                    return label
        
        # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë ˆì´ë¸” ì°¾ê¸° (ë°±ì—…)
        for label in LABELS:
            if label.lower() in predicted_text.lower():
                print(f"Debug - ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë ˆì´ë¸” ì¶”ì¶œ: {label}")
                return label
        
        print(f"âš ï¸ ë ˆì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ í…ìŠ¤íŠ¸: {predicted_text}")
        return 'Unknown'
        
    except Exception as e:
        print(f"âŒ ë ˆì´ë¸” ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return 'Unknown'


# ============================================================================
# ë°ì´í„°ì…‹ ìƒì„± ë° ì „ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================================

def create_text_dataset(data_config: DataConfig, tokenizer, split="train"):
    """
    SFTTrainerì— ì í•©í•œ 'text' í•„ë“œë¥¼ í¬í•¨í•œ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if split == "train":
        csv_path = data_config.train_csv_path
    elif split == "eval":
        csv_path = data_config.eval_csv_path
    else:
        raise ValueError(f"Unknown split: {split}")
    
    # pandasë¡œ ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(csv_path)
    
    # í…ìŠ¤íŠ¸ í¬ë§·íŒ… í•¨ìˆ˜
    def _format_row_to_text(row):
        text = create_train_prompt(
            row[data_config.text_field],
            row[data_config.raw_label_field]
        )
        return {'text': text}

    # ë°ì´í„°í”„ë ˆì„ì„ Datasetìœ¼ë¡œ ë³€í™˜í•˜ê³  'text' í•„ë“œë¥¼ ì¶”ê°€
    dataset = Dataset.from_pandas(df)
    dataset = dataset.map(_format_row_to_text, remove_columns=df.columns.tolist())
    
    return dataset


# ============================================================================
# ëª¨ë¸ ì˜ˆì¸¡ ë° í‰ê°€ í•¨ìˆ˜
# ============================================================================

def predict_label(pred):
    """
    ì˜ˆì¸¡ëœ ë ˆì´ë¸” ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ë ˆì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        pred (torch.Tensor): ì˜ˆì¸¡ëœ ë ˆì´ë¸” ì¸ë±ìŠ¤.
    
    Returns:
        str: ì‹¤ì œ ë ˆì´ë¸”.
    """
    # ìƒìˆ˜ ì •ì˜
    LABELS = ['Abnormal(Nonspecific)', 'Abnormal(Interictal)', 'Abnormal(Ictal)', 'Normal']
    return LABELS[int(pred[0])]

def test_model(model, tokenizer, text):
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        model: ì˜ˆì¸¡ì— ì‚¬ìš©í•  ëª¨ë¸.
        tokenizer: í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì§•í•˜ëŠ” ë° ì‚¬ìš©í•  í† í¬ë‚˜ì´ì €.
        text (str): ì˜ˆì¸¡í•  í…ìŠ¤íŠ¸.
    
    Returns:
        torch.Tensor: ì˜ˆì¸¡ëœ ë ˆì´ë¸” ì¸ë±ìŠ¤.
    """
    try:
        # ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ì„¤ì •
        device = next(model.parameters()).device
        
        # í…ŒìŠ¤íŠ¸ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±(without label)
        test_prompt = create_test_prompt(text)

        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            test_prompt, 
            return_tensors="pt", 
            padding=True, 
            truncation=True,
            max_length=1024
        )
        
        # ì…ë ¥ì„ ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        model.eval()

        outputs = model.generate(
            **inputs,
            max_new_tokens=20,
            do_sample=True,
            top_p=0.9,
            temperature=0.1,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
        )
        
        # í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
        predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print(f'Debug: {predicted_text}')

        return extract_classification_label(predicted_text)
        
    except Exception as e:
        print(f"âŒ test_model ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise e

def evaluate_model_on_test_data(eval_df, model, tokenizer, text_field: str):
    """
    ì£¼ì–´ì§„ ë°ì´í„°í”„ë ˆì„ì˜ ê° í–‰ì— ëŒ€í•´ ëª¨ë¸ì„ í‰ê°€í•˜ê³  ì˜ˆì¸¡ ë ˆì´ë¸”ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    ê° ë°˜ë³µë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        eval_df (pd.DataFrame): í‰ê°€í•  ë°ì´í„°í”„ë ˆì„.
        model: í‰ê°€ì— ì‚¬ìš©í•  ëª¨ë¸.
        tokenizer: í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì§•í•˜ëŠ” ë° ì‚¬ìš©í•  í† í¬ë‚˜ì´ì €.
        text_field (str): í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„.
    
    Returns:
        pd.DataFrame: ì˜ˆì¸¡ ë ˆì´ë¸”ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„.
    """
    print("í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤...")
    
    # ë””ë°”ì´ìŠ¤ í™•ì¸
    device = next(model.parameters()).device

    # Pred ì»¬ëŸ¼ ì´ˆê¸°í™”
    if 'Pred' not in eval_df.columns:
        eval_df['Pred'] = None

    # 'Pred' ì»¬ëŸ¼ì˜ dtypeì„ ëª…ì‹œì ìœ¼ë¡œ objectë¡œ ë³€í™˜í•˜ì—¬ FutureWarning ë°©ì§€
    if eval_df['Pred'].dtype != object:
        eval_df['Pred'] = eval_df['Pred'].astype(object)

    for idx, row in tqdm(eval_df.iterrows(), total=eval_df.shape[0], desc="Evaluating"):
        try:
            predicted_label = test_model(model, tokenizer, row[text_field])
            eval_df.at[idx, 'Pred'] = predicted_label
        except Exception as e:
            print(f"[{idx}] ì˜ˆì™¸ ë°œìƒ: {e}")
            eval_df.at[idx, 'Pred'] = None
        # ê° rowë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬
        cleanup_memory()
    
    return eval_df

def save_test_results(eval_df, output_dir: str, model_name: str, raw_label_field: str):
    """
    í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        eval_df (pd.DataFrame): ì €ì¥í•  í‰ê°€ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„.
        output_dir (str): ì¶œë ¥ ë””ë ‰í† ë¦¬.
        model_name (str): ëª¨ë¸ ì´ë¦„.
        raw_label_field (str): ì‹¤ì œ ë ˆì´ë¸” ì»¬ëŸ¼ëª….
    
    Returns:
        float: í…ŒìŠ¤íŠ¸ ì •í™•ë„
    """
    output_path = Path(output_dir) / "test_results"
    output_path.mkdir(exist_ok=True)
    
    # ì •í™•ë„ ê³„ì‚°
    if 'Pred' in eval_df.columns and raw_label_field in eval_df.columns:
        correct = (eval_df['Pred'] == eval_df[raw_label_field]).sum()
        total = len(eval_df)
        accuracy = correct / total if total > 0 else 0
        
        print(f"\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
        print(f"ì „ì²´ ìƒ˜í”Œ ìˆ˜: {total}")
        print(f"ì •í™•ë„: {accuracy:.4f} ({correct}/{total})")
        
        # MLflowì— ê¸°ë³¸ ë©”íŠ¸ë¦­ ë¡œê¹…
        mlflow.log_metric("test_accuracy", accuracy)
        mlflow.log_metric("test_total_samples", total)
        mlflow.log_metric("test_correct_predictions", correct)
        mlflow.log_metric("test_incorrect_predictions", total - correct)
        mlflow.log_metric("test_error_rate", 1 - accuracy)
        
        # ìƒì„¸í•œ ë¶„ë¥˜ ë¦¬í¬íŠ¸
        try:
            report = classification_report(
                eval_df[raw_label_field], 
                eval_df['Pred'], 
                target_names=['Abnormal(Nonspecific)', 'Abnormal(Interictal)', 'Abnormal(Ictal)', 'Normal'],
                output_dict=True
            )
            
            # ê° í´ë˜ìŠ¤ë³„ ì •í™•ë„ ë¡œê¹…
            for class_name, metrics in report.items():
                if isinstance(metrics, dict) and 'precision' in metrics:
                    mlflow.log_metric(f"test_{class_name}_precision", metrics['precision'])
                    mlflow.log_metric(f"test_{class_name}_recall", metrics['recall'])
                    mlflow.log_metric(f"test_{class_name}_f1", metrics['f1-score'])
                    mlflow.log_metric(f"test_{class_name}_support", metrics['support'])
            
            print(f"\nìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n{classification_report(eval_df[raw_label_field], eval_df['Pred'], target_names=['Normal', 'Abnormal(Nonspecific)', 'Abnormal(Interictal)', 'Abnormal(Ictal)'])}")
            
            # í˜¼ë™ í–‰ë ¬ ë¡œê¹…
            cm = confusion_matrix(eval_df[raw_label_field], eval_df['Pred'], labels=['Normal', 'Abnormal(Nonspecific)', 'Abnormal(Interictal)', 'Abnormal(Ictal)'])
            cm_file = output_path / "confusion_matrix.txt"
            with open(cm_file, 'w') as f:
                f.write(str(cm))
            mlflow.log_artifact(str(cm_file), "confusion_matrix")
            
        except Exception as e:
            print(f"ë¶„ë¥˜ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ê²°ê³¼ ì €ì¥
    result_file = output_path / f"{model_name}_test_results.csv"
    eval_df.to_csv(result_file, index=False)
    
    # MLflowì— ê²°ê³¼ íŒŒì¼ ì•„í‹°íŒ©íŠ¸ë¡œ ë¡œê¹…
    mlflow.log_artifact(str(result_file), "test_results")
    
    print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ {result_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return accuracy


# ============================================================================
# MLflow ì½œë°± í´ë˜ìŠ¤
# ============================================================================

class MLflowCallback(TrainerCallback):
    """MLflow ë©”íŠ¸ë¦­ ë¡œê¹…ì„ ìœ„í•œ ì½œë°± í´ë˜ìŠ¤ (ìë™ Early Stopping ë° ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ í¬í•¨)"""
    
    def __init__(self, patience=10, min_improvement=0.01, min_delta=0.001, model_save_dir=None):
        self.step = 0
        self.patience = patience
        self.min_improvement = min_improvement
        self.min_delta = min_delta
        self.best_loss = float('inf')
        self.patience_counter = 0
        self.loss_history = []
        self.should_stop = False
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
        self.model_save_dir = model_save_dir
        self.best_model_path = None
        self.best_step = 0
        self.model = None
        self.tokenizer = None
        
        print(f"ğŸš€ MLflow ì½œë°± ì´ˆê¸°í™”: patience={patience}, min_improvement={min_improvement}, min_delta={min_delta}")
        if model_save_dir:
            print(f"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìë™ ì €ì¥ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤: {model_save_dir}")
    
    def set_model_and_tokenizer(self, model, tokenizer):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
        self.model = model
        self.tokenizer = tokenizer
    
    def on_train_begin(self, args, state, control, **kwargs):
        """í•™ìŠµ ì‹œì‘ ì‹œ í˜¸ì¶œë©ë‹ˆë‹¤."""
        print("ğŸš€ MLflow ì½œë°±ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ”„ ìë™ Early Stopping ëª¨ë‹ˆí„°ë§ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        if self.model_save_dir:
            print("ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìë™ ì €ì¥ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        """ë¡œê·¸ ì´ë²¤íŠ¸ ì‹œ í˜¸ì¶œë©ë‹ˆë‹¤."""
        if logs:
            for key, value in logs.items():
                if isinstance(value, (int, float)):
                    mlflow.log_metric(key, value, step=self.step)
                    
                    # ì†ì‹¤ ëª¨ë‹ˆí„°ë§ ë° ìë™ Early Stopping ì²´í¬
                    if key == "loss":
                        self.loss_history.append(value)
                        self._check_early_stopping(value)
                        
                        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì²´í¬
                        if self.model_save_dir and self.model is not None:
                            self._check_and_save_best_model(value, state.global_step)
                        
                        # Early Stopping ì¡°ê±´ ë§Œì¡± ì‹œ í•™ìŠµ ì¤‘ë‹¨ ì‹ í˜¸
                        if self.should_stop:
                            print(f"ğŸ›‘ Early Stopping ì¡°ê±´ ë§Œì¡±! Step {self.step}ì—ì„œ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                            control.should_training_stop = True
            
            self.step += 1
    
    def _check_early_stopping(self, current_loss):
        """Early Stopping ì¡°ê±´ì„ ì²´í¬í•©ë‹ˆë‹¤."""
        # ì²« ë²ˆì§¸ ì†ì‹¤ê°’ì€ ê±´ë„ˆë›°ê¸°
        if len(self.loss_history) <= 1:
            return
        
        # ì´ì „ ì†ì‹¤ê°’ê³¼ ë¹„êµ
        previous_loss = self.loss_history[-2]
        
        # ê°œì„  ì—¬ë¶€ í™•ì¸ (ìµœì†Œ ë³€í™”ëŸ‰ ê³ ë ¤)
        if current_loss < self.best_loss - self.min_delta:
            self.best_loss = current_loss
            self.patience_counter = 0
            print(f"âœ… ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! Loss: {self.best_loss:.4f} (Step: {self.step})")
        else:
            self.patience_counter += 1
            print(f"âš ï¸ ê°œì„  ì—†ìŒ ({self.patience_counter}/{self.patience}): Loss {current_loss:.4f} vs Best {self.best_loss:.4f}")
            
            # Early Stopping ì¡°ê±´ í™•ì¸
            if self.patience_counter >= self.patience:
                print(f"ğŸ›‘ {self.patience}ë²ˆ ì—°ì† ê°œì„  ì—†ìŒ. Early Stopping ì¡°ê±´ ë§Œì¡±!")
                self.should_stop = True
    
    def _check_and_save_best_model(self, current_loss, global_step):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤."""
        if current_loss < self.best_loss - self.min_delta:
            try:
                # ì´ì „ ìµœê³  ëª¨ë¸ ì‚­ì œ
                if self.best_model_path and self.best_model_path.exists():
                    import shutil
                    shutil.rmtree(self.best_model_path)
                    print(f"ğŸ—‘ï¸ ì´ì „ ìµœê³  ëª¨ë¸ ì‚­ì œ: {self.best_model_path}")
                
                # ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥
                self.best_model_path = Path(self.model_save_dir) / f"best_model_step_{global_step}_loss_{current_loss:.4f}"
                self.best_model_path.mkdir(parents=True, exist_ok=True)
                
                # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥
                self.model.save_pretrained(str(self.best_model_path))
                self.tokenizer.save_pretrained(str(self.best_model_path))
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                metadata = {
                    "step": global_step,
                    "loss": current_loss,
                    "timestamp": time.time(),
                    "model_type": "unsloth_best"
                }
                
                import json
                with open(self.best_model_path / "metadata.json", "w") as f:
                    json.dump(metadata, f, indent=2)
                
                self.best_step = global_step
                print(f"ğŸ’¾ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {self.best_model_path}")
                print(f"   Step: {global_step}, Loss: {current_loss:.4f}")
                
                # MLflowì— ì•„í‹°íŒ©íŠ¸ë¡œ ë¡œê¹…
                mlflow.log_artifact(str(self.best_model_path), f"best_model_step_{global_step}")
                
            except Exception as e:
                print(f"âŒ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def on_train_end(self, args, state, control, **kwargs):
        """í•™ìŠµ ì¢…ë£Œ ì‹œ í˜¸ì¶œë©ë‹ˆë‹¤."""
        print("âœ… MLflow ì½œë°±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        if self.should_stop:
            print(f"ğŸ›‘ Early Stoppingìœ¼ë¡œ ì¸í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"ğŸ“Š ì •ìƒì ìœ¼ë¡œ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ˆ ìµœì¢… ì†ì‹¤: {self.best_loss:.4f}")
        print(f"ğŸ“Š ì†ì‹¤ íˆìŠ¤í† ë¦¬: {len(self.loss_history)} ìŠ¤í…")
        
        if self.best_model_path and self.best_model_path.exists():
            print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {self.best_model_path}")
            print(f"   Step: {self.best_step}, Loss: {self.best_loss:.4f}")
    
    def get_best_model_path(self):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.best_model_path


# ============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ë“¤
# ============================================================================

def run_unsloth_training(
    model_conf: ModelConfig, 
    data_conf: DataConfig, 
    trainer_conf: TrainerConfig,
    mlflow_run=None
) -> None:
    """Unsloth ê¸°ë°˜ ë¶„ë¥˜ íŒŒì¸íŠœë‹ì„ ìˆ˜í–‰í•˜ê³  í•™ìŠµ ì™„ë£Œ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€í•©ë‹ˆë‹¤."""

    global early_stop_requested
    
    print("ğŸš€ Unsloth ê¸°ë°˜ íŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ğŸ”„ ìë™ Early Stoppingì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (4ë¹„íŠ¸ ì–‘ìí™”)
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_conf.model_dict[model_conf.model_id],
        max_seq_length=data_conf.max_seq_length,
        dtype=None,
        load_in_4bit=model_conf.load_in_4bit,
        full_finetuning=False
    )
    
    # 2. QLoRA ì„¤ì •
    model = FastLanguageModel.get_peft_model(
        model,
        r=16,  # LoRA ë­í¬
        lora_alpha=16,
        lora_dropout=0.1,
        bias="none",
        use_gradient_checkpointing="unsloth",  # ë©”ëª¨ë¦¬ ì ˆì•½
        random_state=3407,
    )
    
    # 3. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ (input_ids í¬í•¨)
    print("ğŸ“Š ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤...")
    train_dataset = create_text_dataset(data_conf, tokenizer, "train")
    eval_dataset = create_text_dataset(data_conf, tokenizer, "eval")
    
    # ë°ì´í„°ì…‹ ê²€ì¦
    print("ğŸ” ë°ì´í„°ì…‹ í˜•ì‹ì„ ê²€ì¦í•©ë‹ˆë‹¤...")
    print(f"í›ˆë ¨ ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜: {len(train_dataset)}")
    print(f"ê²€ì¦ ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜: {len(eval_dataset)}")
    print(f"ì²« ë²ˆì§¸ ìƒ˜í”Œ í‚¤: {list(train_dataset[0].keys())}")
    
    # 4. í•™ìŠµ ì¸ì ì„¤ì • (Unsloth í˜¸í™˜ - Early Stopping ì˜µì…˜ ì œê±°)
    training_args = TrainingArguments(
        output_dir=str(trainer_conf.output_dir),
        num_train_epochs=trainer_conf.num_train_epochs,
        per_device_train_batch_size=trainer_conf.per_device_train_batch_size,
        per_device_eval_batch_size=trainer_conf.per_device_eval_batch_size,
        gradient_accumulation_steps=trainer_conf.gradient_accumulation_steps,
        learning_rate=trainer_conf.learning_rate,
        weight_decay=trainer_conf.weight_decay,
        fp16=trainer_conf.fp16,
        bf16=trainer_conf.bf16,
        optim="adamw_8bit",  # Unsloth ê¶Œì¥
        save_steps=trainer_conf.save_steps,
        logging_steps=trainer_conf.logging_steps,
        max_grad_norm=trainer_conf.max_grad_norm,
        max_steps=trainer_conf.max_steps,
        warmup_ratio=trainer_conf.warmup_ratio,
        group_by_length=trainer_conf.group_by_length,
        lr_scheduler_type=trainer_conf.lr_scheduler_type,
        report_to="none",  # MLflowì™€ ì¶©ëŒ ë°©ì§€
        gradient_checkpointing=True,  # Unslothì™€ í˜¸í™˜
        seed=3407,
    )
    
    # 5. SFTTrainer êµ¬ì„± ë° í•™ìŠµ
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        dataset_text_field="text",
        max_seq_length=data_conf.max_seq_length,
        args=training_args,
        packing=True,  # ì‹œí€€ìŠ¤ íŒ¨í‚¹ìœ¼ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
        packing_efficiency=0.95,
    )
    
    print("ğŸš€ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ğŸ’¡ MLflow UIì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì†ì‹¤ì„ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”:")
    print(f"   mlflow ui --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlruns/mlruns.db")
    print("ğŸ”„ ìë™ Early Stoppingì´ í™œì„±í™”ë˜ì–´ ì†ì‹¤ì´ ìˆ˜ë ´í•˜ë©´ ìë™ìœ¼ë¡œ ì¤‘ë‹¨ë©ë‹ˆë‹¤.")
    
    # MLflow ì½œë°± ì¶”ê°€ (ìë™ Early Stopping)
    mlflow_callback = MLflowCallback(patience=20, min_improvement=0.005, min_delta=0.0005, model_save_dir=trainer_conf.output_dir)
    trainer.add_callback(mlflow_callback)
    mlflow_callback.set_model_and_tokenizer(model, tokenizer) # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •
    
    try:
        # í•™ìŠµ ì‹¤í–‰
        train_result = trainer.train()
        
        if early_stop_requested:
            print("ğŸ›‘ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        elif mlflow_callback.should_stop:
            print("ğŸ›‘ ìë™ Early Stoppingìœ¼ë¡œ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("âœ… í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # í•™ìŠµ ê²°ê³¼ MLflowì— ë¡œê¹…
        if train_result:
            mlflow.log_metric("train_loss", train_result.training_loss)
            mlflow.log_metric("train_runtime", train_result.metrics.get("train_runtime", 0))
            mlflow.log_metric("train_samples_per_second", train_result.metrics.get("train_samples_per_second", 0))
            mlflow.log_metric("training_completed", 1 if not early_stop_requested and not mlflow_callback.should_stop else 0)
            mlflow.log_metric("early_stopping_triggered", 1 if mlflow_callback.should_stop else 0)
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ í‚¤ë³´ë“œ ì¸í„°ëŸ½íŠ¸ë¡œ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        early_stop_requested = True
        mlflow.log_metric("training_interrupted", 1)
    
    # 6. ëª¨ë¸ ì €ì¥
    output_path = Path(trainer_conf.output_dir) / "unsloth_model"
    model.save_pretrained(str(output_path), push_to_hub=False)
    tokenizer.save_pretrained(str(output_path), push_to_hub=False)
    
    # MLflowì— ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ë¡œê¹…
    mlflow.log_artifact(str(output_path), "model")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´ í‘œì‹œ
    best_model_path = mlflow_callback.get_best_model_path()
    if best_model_path and best_model_path.exists():
        print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´:")
        print(f"   ê²½ë¡œ: {best_model_path}")
        print(f"   Step: {mlflow_callback.best_step}")
        print(f"   Loss: {mlflow_callback.best_loss:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ìµœì¢… ëª¨ë¸ë¡œ ë³µì‚¬ (ì„ íƒì‚¬í•­)
        import shutil
        final_best_path = Path(trainer_conf.output_dir) / "best_model_final"
        if final_best_path.exists():
            shutil.rmtree(final_best_path)
        shutil.copytree(best_model_path, final_best_path)
        print(f"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ {final_best_path}ì— ìµœì¢… ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # MLflowì— ìµœì¢… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œê¹…
        mlflow.log_artifact(str(final_best_path), "best_model_final")
    
    if early_stop_requested or mlflow_callback.should_stop:
        print(f"ğŸ›‘ ì¤‘ë‹¨ëœ Unsloth ëª¨ë¸ì´ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"âœ… ì™„ë£Œëœ Unsloth ëª¨ë¸ì´ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("âœ… í•™ìŠµ ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

def run_unsloth_testing(
    model_conf: ModelConfig,
    data_conf: DataConfig,
    trainer_conf: TrainerConfig,
    model_path: Path = None,
    mlflow_run=None
) -> float:
    """í•™ìŠµ ì™„ë£Œëœ Unsloth ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
    
    print("ğŸ§ª Unsloth ëª¨ë¸ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìš°ì„  íƒìƒ‰
    best_model_path = None
    if model_path:
        # best_model_final ë””ë ‰í† ë¦¬ í™•ì¸
        final_best_path = model_path / "best_model_final"
        if final_best_path.exists():
            best_model_path = final_best_path
            print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: {best_model_path}")
        else:
            # ê¸°ì¡´ unsloth_model ë””ë ‰í† ë¦¬ ì‚¬ìš©
            best_model_path = model_path / "unsloth_model"
            print(f"ğŸ“¥ ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {best_model_path}")
    
    if not best_model_path or not best_model_path.exists():
        print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {best_model_path}")
        return 0.0
    
    try:
        # 1. ì €ì¥ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        print(f"ğŸ“¥ ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤: {best_model_path}")
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=str(best_model_path),
            max_seq_length=data_conf.max_seq_length,
            dtype=None,
            load_in_4bit=model_conf.load_in_4bit,
            full_finetuning=False
        )
        
        # ë©”íƒ€ë°ì´í„° í™•ì¸ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì¸ ê²½ìš°)
        metadata_path = best_model_path / "metadata.json"
        if metadata_path.exists():
            import json
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
            print(f"ğŸ“Š ëª¨ë¸ ë©”íƒ€ë°ì´í„°:")
            print(f"   Step: {metadata.get('step', 'N/A')}")
            print(f"   Loss: {metadata.get('loss', 'N/A')}")
            print(f"   íƒ€ì…: {metadata.get('model_type', 'N/A')}")
        
        # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
        if data_conf.test_csv_path and data_conf.test_csv_path.exists():
            print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í‰ê°€í•©ë‹ˆë‹¤...")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
            test_df = pd.read_csv(data_conf.test_csv_path)
            
            # ì»¬ëŸ¼ëª… í™•ì¸ ë° í†µì¼
            if data_conf.text_field not in test_df.columns:
                print(f"ê²½ê³ : '{data_conf.text_field}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {test_df.columns.tolist()}")
                return 0.0
            
            # ë ˆì´ë¸” ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if data_conf.raw_label_field not in test_df.columns:
                print(f"ê²½ê³ : '{data_conf.raw_label_field}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return 0.0
            
            # ëª¨ë¸ í‰ê°€
            test_df_with_predictions = evaluate_model_on_test_data(
                test_df, model, tokenizer, data_conf.text_field
            )
            
            # ê²°ê³¼ ì €ì¥
            model_name = Path(trainer_conf.output_dir).name
            test_accuracy = save_test_results(test_df_with_predictions, trainer_conf.output_dir, model_name, data_conf.raw_label_field)
            
            # ìµœì¢… ì„±ëŠ¥ ìš”ì•½
            print(f"\nğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f}")
            
            return test_accuracy
            
        elif data_conf.test_csv_path:
            print(f"ê²½ê³ : í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_conf.test_csv_path}")
            return 0.0
        
        else:
            print("í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œê°€ ì œê³µë˜ì§€ ì•Šì•„ í…ŒìŠ¤íŠ¸ í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return 0.0
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 0.0
    
    finally:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        cleanup_memory()

def run_unsloth_train_testing(
    model_conf: ModelConfig, 
    data_conf: DataConfig, 
    trainer_conf: TrainerConfig,
    mlflow_run=None
) -> float:
    """
    Unsloth ê¸°ë°˜ íŒŒì¸íŠœë‹ì„ ìˆ˜í–‰í•˜ê³  í•™ìŠµ ì™„ë£Œ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€í•˜ëŠ” í†µí•© í•¨ìˆ˜
    
    Args:
        model_conf: ëª¨ë¸ ì„¤ì •
        data_conf: ë°ì´í„° ì„¤ì •  
        trainer_conf: íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        mlflow_run: MLflow ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
    
    Returns:
        float: í…ŒìŠ¤íŠ¸ ì •í™•ë„
    """
    
    print("ğŸš€ Unsloth íŒŒì¸íŠœë‹ ë° í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("=" * 60)
    
    try:
        # 1ë‹¨ê³„: ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹¤í–‰
        print("ğŸ“š 1ë‹¨ê³„: ëª¨ë¸ íŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        run_unsloth_training(
            model_conf=model_conf,
            data_conf=data_conf, 
            trainer_conf=trainer_conf,
            mlflow_run=mlflow_run
        )
        
        print("âœ… íŒŒì¸íŠœë‹ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("=" * 60)
        
        # 2ë‹¨ê³„: í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        print("ğŸ§ª 2ë‹¨ê³„: í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        model_path = Path(trainer_conf.output_dir)
        
        test_accuracy = run_unsloth_testing(
            model_conf=model_conf,
            data_conf=data_conf,
            trainer_conf=trainer_conf,
            model_path=model_path,
            mlflow_run=mlflow_run
        )
        
        print("âœ… í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("=" * 60)
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        print("ğŸ¯ ìµœì¢… ê²°ê³¼ ìš”ì•½")
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f}")
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {model_path}")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ: {model_path / 'test_results'}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´ í‘œì‹œ
        best_model_final_path = model_path / "best_model_final"
        if best_model_final_path.exists():
            print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê²½ë¡œ: {best_model_final_path}")
            # ë©”íƒ€ë°ì´í„° ì½ê¸°
            metadata_path = best_model_final_path / "metadata.json"
            if metadata_path.exists():
                import json
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                print(f"   Step: {metadata.get('step', 'N/A')}")
                print(f"   Loss: {metadata.get('loss', 'N/A')}")
        
        # MLflowì— ìµœì¢… ë©”íŠ¸ë¦­ ë¡œê¹…
        if mlflow_run:
            mlflow.log_metric("final_test_accuracy", test_accuracy)
            mlflow.log_metric("training_and_testing_completed", 1)
            mlflow.set_tag("pipeline_status", "completed")
        
        return test_accuracy
        
    except Exception as e:
        print(f"âŒ íŒŒì¸íŠœë‹ ë° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
        # MLflowì— ì˜¤ë¥˜ ë¡œê¹…
        if mlflow_run:
            mlflow.log_metric("training_and_testing_failed", 1)
            mlflow.set_tag("pipeline_status", "failed")
            mlflow.set_tag("error_message", str(e))
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        cleanup_memory()
        raise e
    
    finally:
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        cleanup_memory()
        print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")