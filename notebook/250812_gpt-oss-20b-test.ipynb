{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# 1. 모델과 토크나이저를 4비트(4-bit)로 로드합니다.\n",
    "#    최대 시퀀스 길이를 1024로 설정하여 VRAM 사용량을 줄입니다.\n",
    "max_seq_length = 1024\n",
    "model_name = \"unsloth/gpt-oss-7b\" # 더 작은 모델로 빠른 테스트가 가능합니다.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True, # 4비트 양자화\n",
    "    _initialize_weights = False, # 모델 가중치 초기화 오류 방지\n",
    ")\n",
    "\n",
    "# 2. 모델을 QLoRA에 맞게 설정합니다.\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # LoRA 랭크\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", # 메모리 절약\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# 3. 데이터셋을 준비합니다. (alpaca 예시)\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instruction = \"주어진 텍스트를 긍정, 부정으로 분류하세요.\" # 분류 작업에 대한 지시문\n",
    "    texts = []\n",
    "    for text, label in zip(examples[\"text\"], examples[\"label\"]):\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Text:\\n{text}\\n\\n### Classification:\\n{label}\"\n",
    "        texts.append(prompt)\n",
    "    return {\"text\": texts,}\n",
    "\n",
    "# 기존 노트북 코드의 이 부분만 수정하면 됩니다.\n",
    "dataset = load_dataset(\"your_text_classification_dataset.csv\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# 4. 학습 매개변수를 설정하고 학습을 시작합니다.\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1, # VRAM 최적화를 위한 배치 사이즈\n",
    "        gradient_accumulation_steps = 4, # 효과적인 배치 사이즈 4\n",
    "        max_steps = 100, # 예시 학습 스텝 수\n",
    "        learning_rate = 2e-4,\n",
    "        bf16 = True,\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"outputs\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        seed = 3407,\n",
    "    ),\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 5. 파인튜닝된 모델을 저장합니다.\n",
    "model.save_pretrained(\"lora_model\", push_to_hub=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6177b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
