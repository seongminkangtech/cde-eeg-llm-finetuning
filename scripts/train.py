#!/usr/bin/env python3
"""
CDE-EEG-LLM íŒŒì¸íŠœë‹ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python scripts/train.py --method hf          # Hugging Face ê¸°ë°˜
    python scripts/train.py --method unsloth     # Unsloth ê¸°ë°˜
    python scripts/train.py --config configs/training.yaml  # ì„¤ì • íŒŒì¼ ì‚¬ìš©
"""

from dotenv import load_dotenv
load_dotenv()

import argparse
import sys
import os
from pathlib import Path
import mlflow
import mlflow.pytorch
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.training.config import TrainingConfig
from src.training.trainer_hf import run_hf_training
from src.training.trainer_unsloth import run_unsloth_training, run_unsloth_testing, run_unsloth_train_testing

def setup_environment(training_config: TrainingConfig):
    """í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê³  ìºì‹œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    training_config.cache.create_cache_directories()

    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    env_vars = training_config.cache.get_environment_variables()
    for key, value in env_vars.items():
        os.environ[key] = value

    print("ğŸ”§ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ:")
    for key, value in env_vars.items():
        print(f"   {key}: {value}")


def setup_mlflow_experiment(training_config: TrainingConfig, method: str):
    """MLflow ì‹¤í—˜ì„ ì„¤ì •í•˜ê³  ì‹œì‘í•©ë‹ˆë‹¤."""
    
    # MLflow ì„¤ì •
    mlflow.set_tracking_uri(training_config.mlflow.tracking_uri)
    
    # ì‹¤í—˜ ì´ë¦„ ì„¤ì •
    experiment_name = f"{training_config.mlflow.experiment_name}_{method.upper()}"
    mlflow.set_experiment(experiment_name)
    
def log_training_params(training_config: TrainingConfig, method: str):
    """MLflowì— í•™ìŠµ íŒŒë¼ë¯¸í„°ë“¤ì„ ë¡œê¹…í•©ë‹ˆë‹¤."""
    
    # ëª¨ë¸ ê´€ë ¨ íŒŒë¼ë¯¸í„°
    mlflow.log_params({
        "model_id": training_config.model.model_id,
        "method": method,
        "num_labels": training_config.model.num_labels,
        "use_qlora": training_config.model.use_qlora,
        "load_in_4bit": training_config.model.load_in_4bit,
        "max_seq_length": training_config.data.max_seq_length,
        "num_train_epochs": training_config.trainer.num_train_epochs,
        "learning_rate": training_config.trainer.learning_rate,
        "batch_size": training_config.trainer.per_device_train_batch_size,
        "gradient_accumulation_steps": training_config.trainer.gradient_accumulation_steps,
    })
    
    # ë°ì´í„°ì…‹ ì •ë³´
    mlflow.log_params({
        "train_data": str(training_config.data.train_csv_path),
        "eval_data": str(training_config.data.eval_csv_path),
        "test_data": str(training_config.data.test_csv_path),
        "text_field": training_config.data.text_field,
        "label_field": training_config.data.raw_label_field,
    })
    
    # ìºì‹œ ì •ë³´
    mlflow.log_params({
        "unsloth_cache_dir": str(training_config.cache.unsloth_cache_dir),
        "hf_home": str(training_config.cache.huggingface_cache_dir),
        "transformers_cache": str(training_config.cache.transformers_cache_dir),
        "datasets_cache": str(training_config.cache.datasets_cache_dir),
    })


def load_training_config(config_path: Path, model_id: str, method: str, project_root: Path) -> TrainingConfig:
    """ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤. ì„¤ì • íŒŒì¼ì´ ì œê³µëœ ê²½ìš° YAMLì—ì„œ ë¡œë“œí•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."""
    
    if config_path and config_path.exists():
        print(f"ğŸ“‹ ì„¤ì • íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ: {config_path}")
        return TrainingConfig.from_yaml(config_path, model_id, method, project_root)
    else:
        print("ğŸ“‹ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        return TrainingConfig.create_default_config(model_id, method, project_root)


def main():
    """ë©”ì¸ í•¨ìˆ˜."""
    parser = argparse.ArgumentParser(description="CDE-EEG-LLM íŒŒì¸íŠœë‹")
    parser.add_argument(
        "--method",
        choices=["hf", "unsloth"],
        default="unsloth",
        help="íŒŒì¸íŠœë‹ ë°©ë²• ì„ íƒ (ê¸°ë³¸ê°’: unsloth)"
    )
    parser.add_argument(
        "--model-id",
        type=str,
        default="gpt-oss-20b",
        help="ì‚¬ìš©í•  ëª¨ë¸ ID (ê¸°ë³¸ê°’: gpt-oss-20b)"
    )
    parser.add_argument(
        "--config",
        type=str,
        help="ì„¤ì • íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)"
    )
    
    args = parser.parse_args()
    
    # ğŸš¨ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
    project_root = Path(__file__).parent.parent
    
    # ğŸš¨ ì„¤ì • íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
    config_path = None
    if args.config:
        config_path = Path(args.config)
        if not config_path.is_absolute():
            config_path = project_root / config_path
    
    # ğŸš¨ í†µí•© ì„¤ì • ìƒì„±
    training_config = load_training_config(config_path, args.model_id, args.method, project_root)
    
    # ğŸš¨ í™˜ê²½ ë³€ìˆ˜ë¥¼ ê°€ì¥ ë¨¼ì € ì„¤ì •
    setup_environment(training_config)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ë™ì  ì„¤ì •
    training_config.trainer.output_dir = Path(f"output/{training_config.model.model_id}_{args.method}_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    
    print(f"ğŸš€ CDE-EEG-LLM íŒŒì¸íŠœë‹ ì‹œì‘")
    print(f"ğŸ“‹ ë°©ë²•: {args.method.upper()}")
    print(f"ğŸ§  ëª¨ë¸: {training_config.model.model_id}")
    print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {training_config.data.train_csv_path}")
    print(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {training_config.data.eval_csv_path}")
    print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„°: {training_config.data.test_csv_path}")
    print(f"ğŸ’¾ ì¶œë ¥: {training_config.trainer.output_dir}")
    print(f"ğŸ”¬ MLflow ì‹¤í—˜: {training_config.mlflow.experiment_name}")
    print("-" * 50)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    training_config.trainer.output_dir.mkdir(parents=True, exist_ok=True)
    
    # MLflow ì‹¤í—˜ ì„¤ì •
    setup_mlflow_experiment(training_config, args.method)
    
    try:
        # MLflowì— íŒŒë¼ë¯¸í„° ë¡œê¹…
        log_training_params(training_config, args.method)
        
        if args.method == "hf":
            print("ğŸ”„ Hugging Face ê¸°ë°˜ íŒŒì¸íŠœë‹ ì‹œì‘...")
            run_hf_training(training_config.model, training_config.data, training_config.trainer)
        elif args.method == "unsloth":
            print("âš¡ Unsloth ê¸°ë°˜ íŒŒì¸íŠœë‹ ì‹œì‘...")
            run_unsloth_train_testing(training_config.model, training_config.data, training_config.trainer)
        
        print("âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!")
        
        # ìµœì¢… ëª¨ë¸ ë“±ë¡ - ì‹¤ì œ ì €ì¥ ê²½ë¡œ ì‚¬ìš©
        if training_config.mlflow.log_models:
            # ğŸ¯ ì‹¤ì œ ì €ì¥ ê²½ë¡œ ì‚¬ìš©
            if args.method == "unsloth":
                model_path = training_config.trainer.output_dir / "final_model"
            else:
                model_path = training_config.trainer.output_dir / "final_model"
                
            if model_path.exists():
                mlflow.pytorch.log_model(
                    pytorch_model=model_path,
                    artifact_path="model",
                    registered_model_name=f"cde-eeg-llm-{args.method}-{training_config.model.model_id}"
                )
                print(f"ğŸ·ï¸ ëª¨ë¸ì´ MLflowì— ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤: cde-eeg-llm-{args.method}-{training_config.model.model_id}")
            else:
                print(f"âš ï¸ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        
        mlflow.log_param("status", "completed",)

    except Exception as e:
        print(f"âŒ íŒŒì¸íŠœë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        mlflow.log_param("status", "failed")
        mlflow.log_param("error", str(e))
        sys.exit(1)
    
    finally:
        print(f"ğŸ”¬ MLflow ì‹¤í—˜ ì™„ë£Œ")


if __name__ == "__main__":
    main()

